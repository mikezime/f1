
AWSBigDataCertification


| Question | Answer |
| --- | --- |
| In DynamoDB, concerning the GSI (Global Secondary Index)... Can the partition key and sort key be different than the base table? | Yes |
| DynamoDB -- LSI (Local Secondary Index) -- Can the partition key and sort key be different than the base table? | No |
| What is the formula for DynamoDB partition size calculation? | ( readCapacityUnits / 3,000 ) + ( writeCapacityUnits / 1,000 ) |
| What are 2 ways to ensure that the objects inserted into the DynamoDB tables are uniformly distributed via the partitions created in DynamoDB? | - use Random Suffixes as sharding technique<br>- use Calculated Suffixes as a sharding technique |
| When using the following data access patterns for a DynamoDB table, what 2 things should you definitely do?Data Access Patterns (time series data):1) Data is uploaded to the table via an application2) The data is heavily used within a week's time from the ingestion of data3) After a week's time, the data is no longer used | When using the following data access patterns for a DynamoDB table, what 2 things should you definitely do?<br><br>Data Access Patterns (time series data):<br>1) Data is uploaded to the table via an application<br>2) The data is heavily used within a week's time from the ingestion of data<br>3) After a week's time, the data is no longer used- create tables based on a weekly basis, while ensuring a high read and write capacity for these tables<br>- change read and write capacity to a lower value after a week's time for the table |
| What should you do if you need to use DynamoDB for your data store, but some items may exceed the allowable data item size? | - compress the data items if possible<br>- store the data items in S3 and place a link as an attribute in DynamoDB |
| Given the following scenario:A company is making use of Kinesis Data Streams to process millions of records. Enough shards have been assigned to the stream. The KPL library is being used in an application to submit the requests. A Lambda function is being used to process the requests. During the initial stages of execution, there is a latency of around 150 to 200% being recorded in Cloudwatch for the consumption of records. The Lambda function is showing no throttling errors and the memory and time is being consumed as per standard specifications.What might be the most likely underlying problem? | Given the following scenario:<br><br>A company is making use of Kinesis Data Streams to process millions of records. Enough shards have been assigned to the stream. The KPL library is being used in an application to submit the requests. A Lambda function is being used to process the requests. During the initial stages of execution, there is a latency of around 150 to 200% being recorded in Cloudwatch for the consumption of records. The Lambda function is showing no throttling errors and the memory and time is being consumed as per standard specifications.<br><br>What might be the most likely underlying problem?the throughput for the DynamoDB table is being exceeded |
| When using DynamoDB for storing billions of rows, what are 2 good practices when it comes to querying the data? | - try to use the Scan operation with filter expressions<br>- set a smaller page size for the Scan operation<br><br>Note: Because a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. |
| If you already have a DynamoDB table defined and items already exist in the table and now you want to encrypt it at rest, what should you do? | - you must create a NEW table that has encryption enabled<br><br>Note: Encryption at rest can be enabled only when you are creating a NEW DynamoDB table. Currently, you can't enable encryption at rest on an existing table. Also, after encryption at rest is enabled, it can't be disabled. |
| If you need an automated way to store real-time aggregated time series data in DynamoDB, what should you use? | Amazon Kinesis Aggregators framework<br><br>Note: Amazon Kinesis Aggregators is a Java framework that enables the automatic creation of real-time aggregated time series data from Amazon Kinesis streams. |
| What should you use if you need a data store that is highly durable and available that supports both structured and un-structured data and the data will continuously grow? | - use S3 + DynamoDB<br><br>Note:<br>- You can also take advantage of S3 to store large attribute values that cannot fit in a DynamoDB item. You can store them as an object in Amazon S3 and then store the object identifier in your DynamoDB item... <br>- Also, you can use the object metadata support in Amazon S3 to provide a link back to the parent item in DynamoDB. Store the primary key value of the item as Amazon S3 metadata of the object in Amazon S3. Doing this often helps with maintenance of the Amazon S3 objects. |
| What should you use if you need to scale based on demand as well as ensure that the data from the DynamoDB table gets continually replicated to a DynamoDB table in another region? | - use DynamoDB Streams<br>- use the KCL Library to store the data in the destination table |
| If you need to have single-digit milliseconds to data in the DynamoDB table, and also need to ensure that the application hosted on EC2 uses the right security credentials to access the DynamoDB table, what configuration should you use? | - use DAX in-memory cache<br>- attach an IAM Role to the EC2 Instance for accessing the DynamoDB table |
| What should you do if you need to port data from DynamoDB to Redshift? | - ensure that empty attribute values in DynamoDB are properly treated<br>- ensure the data type matches between engines |
| What should you do if you need to store millions of messages being sent from IoT devices, and need to have a durable and highly available data store in AWS to store the incoming messages, and need the data store to scale without much intervention? | - create DynamoDB tables to hold the messages from the IoT devices<br>- create IoT rules to transfer the data to DynamoDB |
| What should you use if need to run ad-hoc queries for data in S3? | - use Athena |
| When using Athena to create tables based on the files in S3, what actions should you take? | - ensure that a Lambda function is in place to remove any unwanted files from the S3 bucket<br>- in Athena, use a trailing slash for your folder or bucket in the LOCATION clause |
| What technique should be used when using Athena to create tables from different source formats on S3? | - use SerDe during table creation<br><br>Note: SerDe (Serializer/Deserializer) is a way in which Athena interacts with data in various formats. The SerDe is defined within the table creation query in Athena. |
| What should you do if you have large data sets defined in S3 and are using AWS Athena to query the data sets, and need to improve query performance while ensuring that cost is not increased? | - split the data set<br>- use the CREATE TABLE AS SELECT statement to create the Athena tables |
| What should you use if need to allow on-premise Active Directory users to use Athena for querying purposes? | - use AWS Security Token Service (STS)<br><br>The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). |
| What should you use when storing large data sets on S3 and you need to use a serverless service for managing the SQL queries, called using JDBC and ODBC drivers? | - use Athena |
| If you need encrypted datasets for S3 and/or encrypted query results for Athena, what are the 3 supported tactics? | - Server side encryption with an Amazon S3-managed key (SSE-S3)<br>- Server-side encryption with a AWS KMS-managed key (SSE-KMS)<br>- Client-side encryption with a AWS KMS-managed key (CSE-KMS)<br><br>Note: Athena does not support customer-managed key (CSE-CMK) |
| What should you do if you need to run queries in Athena on a subset of S3 data? | - create a View<br><br>Note:. For example, you can create a table with a subset of columns from the original table to simplify querying data. OR, Combine multiple tables in one query. When you have multiple tables and want to combine them with UNION ALL, you can create a view with that expression to simplify queries against the combined tables. |
| What should you use if you need to connect business tools to Athena? | - use ODBC<br>- use JDBC |
| What should you do if you're using Athena with a large number of data sets in S3 and need to improve the performance of the underlying queries? | - partition the data<br><br>Note: By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. Athena leverages Hive for partitioning data. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. |
| What should you do if you need to analyze IPs for requests coming into an Amazon Application Load Balancer that sits in front of a number of EC2 instances? | - enable logs for the Application Load Balancer<br>- use Athena to query the log |
| Why might you receive ProvisionedThroughputExceededException when using Kinesis Data Streams with DynamoDB? | - need to increase the provisioned throughput for the DynamoDB table<br>- KCL -- polling interval is too short |
| How can you ensure the strict ordering of records sent and processed in the Kinesis Data Stream? | - use PutRecord (singular) API command, specify the same Partition Key for all PutRecord calls for these records, use the SequenceNumberForOrdering parameter on the PutRecord method. <br><br>Note: all data records with the same partition key map to the same shard within the stream.<br><br>Note: PutRecords (plural) cannot guarantee the ordering of records |
| When you need to ensure that data is distributed across the shards of Kinesis Data Streams, which of the following aspect of the data record helps achieve this? | - Partition Key |
| How long does Kinesis Streams store data? | - Kinesis data stream stores records from 24 hours by default, up to 168 hours |
| What should you do if you need to stream logs using the Kinesis Data Firehose service and need to decide on a data store when the resulting files on the data store will be heavily queried over a week's period time and after that can be archived for future analysis? | - ensure the destination for Kinesis Data Firehose is marked as S3<br>- create a lifecycle policy for S3 to archive older files<br><br>Note: A lifecycle configuration is a set of rules that define actions that S3 applies to a group of objects. There are two types of actions:<br>1) Transition actions—Define when objects transition to another storage class. For example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them, or archive objects to the GLACIER storage class one year after creating them.<br>2) Expiration actions—Define when objects expire. S3 deletes expired objects on your behalf. |
| What should you do if you need to stream data from various sources and ensure that data is encrypted at rest in Kinesis? | - use client-side encryption for the data at the Producer side; AND / OR<br>- use server-side encryption and AWS Key Management Service (KMS) keys. (new feature as of 2017) |
| What is used to specify the location from where you can start reading records from a Kinesis stream? | - ShardIterator<br><br>Note: If there are no records available in the portion of the shard that the iterator points to, GetRecords returns an empty list. It might take multiple calls to get to a portion of the shard that contains records. |
| What should you do if you're using Kinesis Data Streams to stream data, then Lambda functions to process the data, and want to optimize the process? | - customize memory for Lambda<br>- customize batch size for Lambda<br>- customize number of shards for the Kinesis Data Stream |
| What is Kinesis Agent? | - Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams. The agent continuously monitors a set of files and sends new data to your stream. The agent handles file rotation, checkpointing, and retry upon failures. It delivers all of your data in a reliable, timely, and simple manner.<br><br>Note: Kinesis Agent emits CloudWatch metrics to help you better monitor and troubleshoot the streaming process. |
| When using the Kinesis Agent to transfer various logs files from multiple EC2 Instances to Kinesis, how do you ensure that the data gets streamed accordingly? | - ensure that the flows section is modified in the agent configuration file<br><br>Note: In this configuration file, in the "flows" JSON element, specify the files ( "filePattern" ) from which the agent collects data, and the name of the delivery stream ( "deliveryStream" ) to which the agent sends data. |
| What should you do when an EC2 hosted application needs to connect to the Kinesis Data Stream without passing through the Internet? | - use VPC endpoint |
| What should you do, if you need a data store for log files from various EC2 Instances wherein these log files need to be streamed from the various servers and then stored for analysis at a later stage? | - ingest the data using Kinesis and then store the data in S3 |
| When you're using Kinesis Data Firehose to stream log files onto S3 and need to ensure that source data stream for Kinesis Data Firehose is encrypted, what should you do? | - use Kinesis Data Streams as a source <b>*</b> this doesn't make sense to me<br><br>Note: If you have sensitive data, you can enable server-side data encryption when you use Kinesis Data Firehose. But you can only do this if you configure a Kinesis Data Stream as your data source. |
| What are the 4 supported data destinations for Kinesis Data Firehose? | - S3<br>- RedShift<br>- Splunk<br>- ElasticSearch |
| What should be used to analyze a large set of data updates from Kinesis and DynamoDB? | ElasticSearch |
| What should you use when utilizing DynamoDB to ingest and store large amounts of metric related data a large number of log files and want a serverless architecture and don't want to manage the underlying infrastructure and want log analytics? | - use ElasticSearch<br><br>Note: Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. |
| In Redshift, what distribution style required to ensure that the table can be frequently used in join level queries? | - KEY |
| What is the default distribution style in Redshift? | - EVEN |
| What distribution style should you use if the Redshift table does not participate in joins? | - EVEN (default)<br><br>Note: EVEN is the most performant until joins get involved. |
| When using a Redshift Cluster with a Star Schema design (consists of a fact table and dimensions table), which of the following is the ideal distribution style to use for the fact table? | - KEY (Designate both the dimension table's primary key and the fact table's corresponding foreign key as the DISTKEY.)<br><br>Note: Star and snowflake schemas organize around a central fact table that contains measurements for a specific event, such as a sold item. The fact table has foreign key relationships to one or more dimension tables that contain descriptive attribute information for the sold item, such as customer or product. |
| What should you do if you need to add more data to an existing Redshift table? | - use a staging table, then merge the data<br><br>Note: While Redshift does not support a single merge, or upsert, command to update a table from a single data source, you can perform a merge operation by creating a staging table and then using one of the methods described in this section to update the target table from the staging table. |
| What should you do if you're using Redshift and experiencing bad performance on queries involving new data added to an existing table? | - optimize the table with the VACUUM command<br><br>or<br><br>- create a new table and start writing to that one instead |
| When running EXPLAIN queries against tables in a Redshift Cluster, what return value would require the team to re-check on the distribution styles used for the underlying tables? | - DS_DIST_INNER (bad)<br>- DS_DIST_ALL_INNER (bad)<br><br>Note: Conversely, DS_DIST_NONE (good) and DS_DIST_ALL_NONE (good) are what we want to see... These 2 query plan return values indicate that no distribution was required for that step because all of the joins are collocated. |
| What should you do if you receive a new requirement that your Redshift cluster data needs to be encrypted at rest? | - prep step: disable cross-region snapshot copy<br>- prep step: ensure that an OUTAGE interval is kept in mind for the migration process, which is triggered by changing Redshift cluster encryption settings<br>- finally: update Redshift cluster encryption settings<br><br>Note: During the migration operation (required to change encryption), your cluster is available in read-only mode, and the cluster status appears as resizing. If your cluster is configured to enable cross-region snapshot copy, you must disable it before changing encryption. |
| What should you do if some long running Redshift queries are consuming resources, causing other queries to be slow? | - for LEAST impact, modify the WLM (Workload Management) configuration for the cluster |
| What should you use if you need to migrate an on-prem RDMS (example: Oracle database) to Redshift? | - use AWS schema conversion tool<br><br>Note: You can use the AWS Schema Conversion Tool (AWS SCT) to convert your existing database schema from one database engine to another. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL DB instance, an Amazon Aurora DB cluster, an Amazon RDS PostgreSQL DB instance, or an Amazon Redshift cluster. |
| Which Redshift distribution style would be ideal for the following table characteristics?1) The data in the table doesn't change frequently.2) There would be less than 10 millions rows3) The table would not have joins with other tables | Which Redshift distribution style would be ideal for the following table characteristics?<br>1) The data in the table doesn't change frequently.<br>2) There would be less than 10 millions rows<br>3) The table would not have joins with other tables- ALL<br><br>Note: ALL distribution is appropriate only for relatively slow moving tables; that is, tables that are not updated frequently or extensively. |
| What should you do if you need to ensure that a Redshift database can be restored to its original state, in case there are any issues with a change? | - create a manual snapshot |
| What should you do if you need to ensure that the data from a PostgreSQL table gets stored into a Redshift staging table? | - use the extensions available in Postgres and use the dblink facility |
| What warrants the use of EVEN distribution style for the underlying tables in Redshift? | - when a table does not participate in joins<br>- when the table's design is new and there is no clear distinction on how the data will be organized (EVEN is default distribution style) |
| What should you do if you need to ensure that a Redshift Cluster using KMS key for encryption can be moved to another region? | - ensure that a new KMS key is created in the new region<br>- configure a snapshot copy grant for a master key in the destination region<br><br>Note: AWS KMS keys are specific to a region. If you want to enable cross-region snapshot copy for an AWS KMS-encrypted cluster, you must configure a snapshot copy grant for a master key in the destination region |
| What would warrant the use of KEY distribution style for the underlying Redshift tables? | if join queries run against those tables frequently (KEY distribution style reduces traffic between nodes during the join operation)<br><br>Note: In a Redshift table with KEY distribution style, the rows are distributed according to the values in one column. The leader node will attempt to place matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together. |
| What should you do if you need to be able to restrict access to Redshift table data? | - create a view |
| If a few users in a user group in Redshift use up all available resources and this disrupts the quality of service for other users in the group, how should you distribute this user workload? | - create a separate user group and make use of Workload Management |
| If you need Redshift but don't know what cluster size you need, what is the most cost-efficient way to provision? | - create the Redshift Cluster favoring the least number / least powerful nodes minimally required for any estimated workload<br><br>Note: If your storage and performance needs change after you initially provision your cluster, you can resize your cluster. You can scale the cluster in or out by adding or removing nodes. Additionally, you can scale the cluster up or down by specifying a different node type. |
| What are some best practices for Redshift?* this is a horrible flashcard | What are some best practices for Redshift?<br><br><b>*</b> this is a horrible flashcard- use transaction handling for ETL processes that use Redshift commits<br>- extract large result sets from Redshift using the UNLOAD statement |
| What should you use if you need a list of all IP addresses which establish connections to your Redshift cluster? | - use Connection Logs in Redshift<br>- use User Logs in Redshift<br><br>Note: Amazon Redshift logs information in the following log files:<br>- Connection log: authentication attempts, and connections / disconnections<br>- User log: information about changes to database user definitions<br>- User activity log: logs each query before it is run on the database |
| What should you do if you need to ensure that the keys used for encryption for the Redshift cluster are from an on-premise HSM device? | - use client and server certificates to configure a trusted connection between Redshift and your HSM<br>- create a VPN connection between the VPC holding the cluster and the on-premise network |
| What is Redshift Spectrum? | - used for querying nested data in Parquet, ORC, JSON, and Ion file formats (these files can be stored on S3 and used as external tables without having to COPY the data into Redshift) |
| If your Redshift cluster will be hosted in a VPC, what is required to ensure the data can be sent from Kinesis Data Firehose to Redshift? | - If your Amazon Redshift cluster is in a virtual private cloud (VPC), it must be publicly accessible with a public IP address.<br>- Also, grant Kinesis Data Firehose access to your Amazon Redshift cluster by unblocking the Kinesis Data Firehose IP addresses. |
| What is your strategy if you need CSV files stored in S3 to be transferred to Redshift? | - use as many CSVs as possible (to optimize for parallelism during COPY)<br>- use a Redshift node type with an adequate instance size |
| What is best practice to ensure data files are uploaded efficiently to a Redshift cluster from S3? | - use a manifest file for the COPY command<br><br>Note: S3 provides eventual consistency for some operations. Thus, it's possible that new data won't be available immediately after the upload, which can result in an incomplete data load or loading stale data. You can manage data consistency by using a manifest file to load data. |
| What should you do if you need to verify that the data was loaded correctly into Redshift from multiple files in an S3 bucket? | - query the STL_LOAD_COMMITS table<br><br>Note: After the load operation is complete, query the STL_LOAD_COMMITS system table to verify that the expected files were loaded. You should execute the COPY command and load verification within the same transaction so that if there is problem with the load you can roll back the entire transaction. |
| When using S3 for file uploads and Redshift for the data store, how do you automate the way files get loaded into Redshift? | - use Lambda functions to poll the S3 buckets and COPY the content onto Redshift |
| What should you do if you need to UNLOAD data from various Redshift tables, while keeping all unloaded data encrypted at rest? | - use S3 server-side encryption with KMS Keys (SSE-KMS); OR<br>- use S3 server-side encryption default AWS-managed encryption keys (SSE-S3); OR<br>- use client-side encryption with a customer-managed key (CSE-CMK)<br><br>Note: UNLOAD automatically creates files using Amazon S3 server-side encryption with AWS-managed encryption keys (SSE-S3). You can also specify server-side encryption with an AWS Key Management Service key (SSE-KMS) or client-side encryption with a customer-managed key (CSE-CMK).<br><br>Note: UNLOAD doesn't support Amazon S3 server-side encryption using a customer-supplied key (SSE-C). |
| What should you do if Kinesis Data Streams is already being used for an application and there is a new requirement to perform SQL queries on the live data for analytical purposes? | - create an EMR Cluster with Spark installed. Stream the data from Kinesis Data Streams to Spark. Use Spark to perform the queries. |
| What are the EMR node types? | - Master Node<br>- Core Node<br>- Task Node |
| What is the best practice security architecture for EMR? | - SSL between nodes<br>- S3 encryption<br>- EBS volume encryption |
| What should you do if you need to ensure your EMR clusters are transient in nature? | - create the cluster using the EMR API<br>- enable auto-termination on the cluster |
| What should you do if one of your admins needs the ability to login into your EMR nodes? | - allow SSH connections (TCP port 22) via Security Groups |
| What can you use to perform SQL queries on underlying EMR data? | - Hive (Hive scripts use a SQL-like language called Hive Query Language)<br>- Presto (fast SQL query engine -- supports multiple sources) |
| What should you do if your EMR cluster needs a Hive metastore to persist even after the EMR Cluster is terminated? | - start with a MySQL database on the master node (created by Hive by default)<br>- modify the JDBC config in hive-site.xml on the EMR Cluster<br><br>Options for persisting Hive metastore to external metastore:<br>- AWS Glue Data Catalog (Amazon EMR version 5.8.0 or later only).<br>- Amazon RDS<br>- Amazon Aurora |
| What should you do if you need your own implementations of the EMR Mapper and Reducer functions? | - create a Streaming Step for the EMR Cluster<br><br>Note: A Streaming application reads input from standard input and then runs a script or executable (called a mapper) against each input. The result from each of the inputs is saved locally, typically on a Hadoop Distributed File System (HDFS) partition. After all the input is processed by the mapper, a second script or executable (called a reducer) processes the mapper results. The results from the reducer are sent to standard output. You can chain together a series of Streaming Steps, where the output of one step becomes the input of another step.<br><br>Note: The mapper and the reducer can each be referenced as a file or you can supply a Java class. You can implement the mapper and reducer in any of the supported languages, including Ruby, Perl, Python, PHP, or Bash. |
| If your AWS Redshift cluster will be hosted in a VPC, what is required to ensure the data can be sent from Kinesis Data Firehose to Redshift? | - If your Amazon Redshift cluster is in a virtual private cloud (VPC), it must be publicly accessible with a public IP address.<br>- Also, grant Kinesis Data Firehose access to your Amazon Redshift cluster by unblocking the Kinesis Data Firehose IP addresses. |
| What should you do if your EMR cluster has new storage requirements and you need to ensure the new requirements are met with the least effect on the cluster? | - if the replication factor is high, you can reduce it<br>- add more nodes to the cluster<br>- use compression |
| - what can be installed on the EMR Cluster to provide a monitoring dashboard for individual nodes? | - Apache Ganglia (cluster monitoring system) |
| What should you use if you need petabytes of storage for sparse data? | - EMR Cluster with HBase (HBase has efficient storage of sparse data) |
| What should you do if you need to ensure that EMR cluster data is available even after the cluster is torn down? | - use EMRFS<br><br>Note: EMRFS is an implementation of the Hadoop file system used for reading and writing regular files from EMR directly to S3 |
| If you cannot connect to the Master node of your EMR Cluster via ssh, what should be your first step in troubleshooting? | Check the Inbound rules for the Security Group for the Master node. Make sure that security group contains a rule that allows you to connect to the master node using an SSH client over TCP port 22. |
| When using Apache Spark on an EMR Cluster and you need to have interactive data analytics which can be performed on the underlying data, what should you use? | - use Apache Zeppelin (notebook) |
| What should you do if your have a Hadoop Cluster setup using the AWS EMR service and need all data encrypted in transit within the Hadoop Cluster? | - use Hadoop MapReduce Encrypted<br>- secure Hadoop RPC is set to "Privacy"<br>- use SASL |
| If you have an on-premise data store in Oracle and need to import the data into an Amazon EMR Cluster which uses HDFS, what should you use? | - use Sqoop<br><br>Note: Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.<br><br>Note: AWS Schema Conversion Tool wouldn't work for this scenario, since Oracle-to-EMR + HDFS is not supported |
| If you have an EMR Cluster with Hive Job flows and need to be able to create dashboards and analyse data in the EMR Cluster, what can you use? | - use Microstrategy; OR<br>- use Excel; OR<br>- use QlikView; OR<br>- use Tableau<br><br>Note: You can use popular business intelligence tools like Microsoft Excel, MicroStrategy, QlikView, and Tableau with Amazon EMR to explore and visualize your data. Many of these tools require an ODBC (Open Database Connectivity) or JDBC (Java Database Connectivity) driver. |
| What should you use if you need interactive analysis for an EMR cluster? | - use Presto<br><br>Note: Presto is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources. |
| What should you use if you need Petabytes of data migrated from an on-premise environment to AWS? | - use AWS Snowball Device |
| What should you do if you have an EMR Cluster, and need to make sure the cluster always works with the most recent updated data in S3? | - enable consistent view<br><br>Note: Consistent view allows EMR clusters to check for list and read-after-write consistency for S3 objects written by or synced with EMRFS. |
| What should you use if you have a Hadoop cluster setup on top of EMR and need to carry out some Machine Learning algorithms on the existing data? | - use Apache Mahout<br><br>Note: Mahout can use Spark as a backend |
| What should you do if you're setting up an EMR cluster and need to store the metadata in a central repository? | - create a MySQL database in AWS RDS<br>- modify the configuration of the cluster during setup |
| What should you use if you have a large number of Spark jobs that need to run on an EMR cluster and need some way to orchestrate the series of jobs? | - use AWS Step Functions |
| If you're using EMR to process billions of IP-address based rows and need to be able to query the data efficiently, what should you use? | - use HBase and use the IP-address as the underlying key<br><br>Note: Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS. |
| If you need to create a serverless solution which could be used to create visualizations from the data stored in the S3 buckets, what could you use? | - QuickSight<br>- D3.js |
| If you need all S3 data sets encrypted at rest and to ensure that the account owner manages the complete lifecycle of the underlying keys used for encryption, what should you do? | - use S3 server-side encryption with Customer keys<br>- use S3 client-side encryption |
| What should you use if you need to upload different types of data items without responsibility of managing the underlying capacity of the service? | - use S3 |
| What should you do if you need an S3 bucket to comply with PCI DSS compliance standards? | - enable server-side encryption for the S3 bucket<br>- ensure that objects from the S3 bucket are requested only via HTTPS |
| What should you use if you need to verify that a file is unchanged after moving it around in S3? | - use the ETag (usually an MD5, always a hash) |
| What should you do if you need a secure backup and archiving solution wherein documents should be immediately accessible for three months and available for five years for compliance reasons? | - upload data to S3 and use lifecycle policies to move the data into Glacier for long-term archiving |
| What is the most secure and ideal way to authenticate IoT devices with AWS? | - X.509 Certificates |
| What should you use if you need to get the state of a device even if it is disconnected from the IoT service? | - use Device Shadows<br><br>Note: A device's shadow is a JSON document that is used to store and retrieve current state information for a device |
| What is the preferred security configuration for mobile applications? | Amazon Cognito identities |
| What are the 2 preferred authentication method for web and desktop apps? | - IAM<br>- Federated Identities |
| What is the preferred security configuration for CLI commands? | IAM |
| When using the ElasticSearch service, how many master nodes are recommended? | 3 |
| In the context of machine learning, which model is used for a simple case of a Yes or No classification? | - Binary classification model |
| How should you prepare your data to ensure it can be used as Input data for Amazon Machine Learning? | - store the data in S3<br>- use CSV files<br>- use the same data schema for all input files |
| When using the Machine Learning Binary classification model, after the initial evaluation, the AUC is showing a score of 0.51, what does this mean? | - the evaluation is basically guessing at the result<br><br>Note: we want the AUC (Area Under Curve) near 1 |
| What Machine Learning model should you use if you need to determine if reviews written on your web site came from a customer or a bot? | - use Binary Classification Model |
| What should you use if you need to crawl all of the log files generated via CloudTrail? | - use Lambda functions to transform the log files<br>- use Glue for cataloguing the information |
| When using Amazon Kinesis Firehose to stream data into an S3 bucket, and need to transform the data before it can be used for analysis, what should you do? | - use Lambda |
| What should you use if you have data stores on both on-premise and AWS environments and need to create a data lake in AWS and then orchestrate several ETL jobs? | - use AWS S3 for storage of data lake<br>- use combination of Lambda and Step functions |
| When using a plethora of AWS services such as RDS and Redshift and need to have a unified metadata repository for all of these data sources, what should you use? | - use Glue |
| When you're using QuickSight and need a way to compare the values of current sales data against the forecasted sales data, what should you use? | - use KPIs (Key Performance Indicators) |
| If you need to give users the ability to customize the data being used in the visual of the QuickSight dashboard, what should you use? | - use Parameters |
| If you need users to be able to perform multiple iterations of an analysis in QuickSight, what should you use? | - use Stories |
| If a dataset's date column is not being recognized in QuickSight, what should you do? | - change the format of the date column in the dataset |
| What markup languages / file formats are allowed for the data to be uploaded into CloudSearch? | - XML<br>- JSON<br><br>Note: The CloudSearch service allows you to upload any data in the supported formats to quickly add rich search capabilities to your website or application. |
| What should you use if you need a data store on AWS that can be used to store a large number of log files and has the following capabilities? Range searches Term boosting Faceting | What should you use if you need a data store on AWS that can be used to store a large number of log files and has the following capabilities?<br> Range searches<br> Term boosting<br> Faceting- use CloudSearch<br><br>Note: You can use Amazon CloudSearch to index and search both structured data and plain text. <br><br>Amazon CloudSearch features:<br>Full text search with language-specific text processing<br>Boolean search<br>Prefix searches<br>Range searches<br>Term boosting<br>Faceting<br>Highlighting<br>Autocomplete Suggestions |
| If you have a lot of data in many disparate sources such as Hive, Cassandra, Redis, and MongoDB and want to allow users to perform fast queries on these underlying data sources using a standard query language, what should you use? | - use Presto |
| Which AWS Instance Type is good for Machine Learning purposes? | - The C type (Compute Optimized) Instances are good for Machine Learning purposes<br><br>Note: The C-type instance is also good for High performance web servers, scientific modelling, batch processing, distributed analytics, high-performance computing (HPC), machine/deep learning inference, ad serving, highly scalable multiplayer gaming, and video encoding. |
| Which AWS Instance Type is good for ad-hoc work? | The T Instance types are good for ad-hoc work since they have capacity for burstable performances<br><br>Note: T instance types are available as On-Demand Instances, Reserved Instances, and Spot Instances, but not as Scheduled Instances or Dedicated Instances. They are also not supported on a Dedicated Host. |
| What should be used to orchestrate the following ? - Using Apache Hadoop to process web server logs - The logs then need to be cleansed - Finally, the logs need to be delivered to Amazon S3 | What should be used to orchestrate the following ?<br> - Using Apache Hadoop to process web server logs<br> - The logs then need to be cleansed<br> - Finally, the logs need to be delivered to Amazon S3- use AWS DataPipeline |
| If you have an application that writes data to AWS RDS-MySQL and need to push data on a daily basis to archive records from MySQL tables to S3 for future analysis, what should you do? | - use AWS DataPipeline and run a job on a daily basis |
| If you need to store data in AWS, perform analysis on that data, and authenticate using SAML federation, what should you do? | Ingest the data using Kinesis streams. Move the data from Kinesis streams into DynamoDB. Ensure that access is given via STS (Secure Token Service) using temporary access credentials<br><br>Note: The AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users) |
| If you have the following requirements, what should you use? - A data lake that can expand on demand to store patient heart rate information. - A way to ingest the data and store it in the data lake. - A way to catalogue the information | If you have the following requirements, what should you use?<br> - A data lake that can expand on demand to store patient heart rate information.<br> - A way to ingest the data and store it in the data lake.<br> - A way to catalogue the information- use S3 as the data lake, Kinesis to ingest the data, and Glue to catalogue the information |
| Given the following scenario:You're planning to stream log files from EC2 Instances using Kinesis Data Streams and Kinesis Data Firehose. The streaming data will then be parsed using Lambda and stored in Redshift. After testing, you see the amount of data in S3 has increased, and you have to delete the data manually. But, since this process will be triggered on a continual basis, you need to ensure the right step is taken to delete the data in S3 in an automated manner.What should you do? | Given the following scenario:<br><br>You're planning to stream log files from EC2 Instances using Kinesis Data Streams and Kinesis Data Firehose. The streaming data will then be parsed using Lambda and stored in Redshift. <br><br>After testing, you see the amount of data in S3 has increased, and you have to delete the data manually. But, since this process will be triggered on a continual basis, you need to ensure the right step is taken to delete the data in S3 in an automated manner.<br><br>What should you do?- create a Lifecycle Policy for the S3 bucket |
| If you need to ingest all logs from EC2 Instances into a searchable service, what should you do? | - setup Cloudwatch events to stream the data to Kinesis Data Firehose<br>- configure the end destination for the log files as ElasticSearch |
| If you need to use Machine Learning on a dataset but you're missing the target labels required for training, what should be the next step to ensure you get the right results from the Machine Learning analysis? | - assign a team to manually assign labels or you can't use Machine Learning |
| What technology is used by Athena to partition data? | Apache Hive |
| What are the nodes available for transfer of data through Data Pipeline? | - DynamoDBDataNode<br>- RedShiftDataNode<br>- S3DataNode<br>- SqlDataNode<br><br>Note: AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals. Supports S3, RDS, DynamoDB, and EMR. |
| Is Amazon Glacier a node available for transfer of data through Data Pipeline? | No |
| What is the maximum total data read rate for a Kinesis shard? | 2MB/s |
| If a data workload is being managed in Aurora and the data needs to be encrypted at rest, what should you use? | AWS KMS can be used for managing the encryption keys |
| If you need to collect data from multiple Amazon Redshift clusters and consolidate the data into a single central data warehouse, but data must be encrypted at all times while at rest or in flight, what is the most scalable way to do this? | Use AWS KMS data key to run an UNLOAD ENCRYPTED command that stores the data in an unencrypted S3 bucket; run a COPY command to move the data into the target cluster. |
| What is the maximum number of Redshift queries that can run per queue? | 5 |
| If you need to insert or update a RedShift table, what should you do first? | create a staging table |
| What should you do when a Redshift table's performance decreases due to a large number of inserts and updates? | - Add another table and start writing to that one.<br>- optimize with VACUUM command<br><br>Note: VACUUM Resorts rows and reclaims space in either a specified table or all tables in the current database. (Similar to Postgres VACUUM) |
| What 2 types of AWS encryption services are supported by RedShift? | - AWS KMS<br>- AWS Cloud HSM (hardware security module) |
| What 2 services can be used to analyze storage access patterns in S3 buckets? | - S3 Analytics feature<br>- QuickSight |
| What are the requirements to connect QuickSight to S3? | 1) authentication configured (quicksight must be authorized for S3 separately from your user)<br>2) valid manifest file created in the bucket to be accessed<br>3) target file that is described by the manifest file exists, and is available |
| What AWS IoT Core feature allows you to create a persistent virtual version of a device? | Device Shadows |
| What industry standard format is the Device Shadow stored in? | JSON -- used to store and retrieve current state information for a device |
| How can you minimize the impact of numerous read-heavy operations in DynamoDB? | - implement an ElastiCache solution; OR<br>- use DAX<br><br>Note: AWS ElastiCache is Amazon's answer to Redis and Memcached -- requires logic in application.<br><br>Note: With DAX, you can seamlessly integrate a cache layer without having the caching logic in the application.<br><br>Note: If you require sub-millisecond latency, without the headache of managing a cache layer, and your application is Java or Node.js, then DAX is for you. If you already have a Redis or Memcached solution, use ElastiCache. |
| Which DynamoDB stream record view shows the record as it was before it was modified? | OLD_IMAGE |
| What Machine Learning component is used to generate predictions from input data patterns? | Models |
| In the context of Machine Learning, what do you call data where the target answers are already known? | Labeled data |
| Which privileges should be assigned to an IAM user by default? | None.<br><br>Note: Only the permissions required to perform the intended task should be allocated to a new user. (Assign the least permissions possible) |
| What tool is used for transferring data between Amazon S3, Hadoop, HDFS, and RDBMS? | Sqoop |
| What should you do to ensure the EC2 Auto Scaling Group happens automatically on demand? | use SQS queues to decouple the architecture<br><br>Note: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. |
| What is a tool that allows SQL querying of HDFS and S3 file systems? | - definite answer: Presto<br><br>Note: Presto (or PrestoDB) is an open source, distributed SQL query engine, designed from the ground up for fast analytic queries against data of any size. It supports both non-relational sources, such as the Hadoop Distributed File System (HDFS), Amazon S3, Cassandra, MongoDB, and HBase, and relational data sources such as MySQL, PostgreSQL, Amazon Redshift, Microsoft SQL Server, and Teradata.<br><br>Presto can query data where it is stored, without needing to move data into a separate analytics system. Query execution runs in parallel over a pure memory-based architecture, with most results returning in seconds. |
| How are YAML and JSON related? | YAML is a superset of JSON with a more complex specification than JSON. |
| Can you parse YAML with a JSON parser? | No |
| Can you parse JSON with a YAML parser? | Yes |
| What are the BI services supported by amazon? | MicroStrategy<br>PowerBI<br>Tableau<br><br><b>*</b> seems really wrong -- can't corroborate |
| Which AWS security configuration supports supports SSO with existing Google Credentials? | Cognito |
| Which of the EMR Cluster nodes is responsible for running the YARN service? | Master node |
| If you cannot connect to the Master node of your EMR Cluster via ssh, what should be your first step in troubleshooting? | Check the Inbound rules for the Security Group for the Master node. Make sure that security group contains a rule that allows you to connect to the master node using an SSH client over TCP port 22. |
| What are the 4 required things to set up a DynamoDB instance? | - instance name<br>- partition key<br>- read capacity<br>- write capacity<br><br>Note: sort key is often initially configured as well (optional) |
| What is the size limitation of DynamoDB objects (row size limit)? | 400k objects<br><br>Note: the reason for this small size is that DynamoDB's key characteristic is speed |
| What should you use to create alerts that notify you if an EMR cluster is underutilized? | - use CloudWatch |
| Can EMR task nodes be resized? | Yes |
| What are 4 models supported by AWS Machine Learning? | - binary classification<br>- multiclass classification<br>- Linear regression<br>- graph processing |
| What 7 AWS services can you use to load data into RedShift? | - S3<br>- DynamoDB<br>- EMR<br>- Kinesis Data Firehose<br>- Glue<br>- Data Pipeline (helps orchestrate the flow of data)<br>- any SSH-enabled host on Amazon EC2 or on-premises |
| How can you decide whether to use Redshift vs EMR? | By whether or not the data is well-structured.<br><br>Note:<br>- use Redshift when data is well structured (best for relational data)<br>- use EMR when the data is unstructured (EMR can transform the data so it's easier to work with) |
| What are the 3 Redshift Key Distribution options, and what do they do? | EVEN: (Default) Distribute the rows across the slices in a round robin fashion<br><br>KEY: Distribute according to the values in one column<br><br>ALL: Distribute the entire table to every node. |
| If you need Redshift to read data directly from S3, what should you use? | - use Redshift Spectrum<br><br>Note: Redshift doesn't talk directly to S3 (see: COPY command) |
| Can you connect Quicksight to EC2 or on-premise databases? | Yes |
| Will existing data be encrypted when you turn on encryption for Amazon Kinesis Streams? | No |
| How can you keep your DynamoDB tables in sync across multiple AWS Regions? | DynamoDB Streams to a global table<br><br>Note: this will never be strongly consistent |
| Can you configure a Kinesis Data Stream as the source of multiple Kinesis Data Firehose delivery streams? | Yes |
| What is the default retention for DynamoDB Streams? | 24-hour default retention |
| When should you use Kinesis Data Firehose instead of Kinesis Data Streams? | how long can you wait?<br><br>- if you can sustain a data latency of 60 seconds or higher (up to 900 seconds) (configurable), use Kinesis Data Firehose <br><br>otherwise,<br><br>- if you need latency less than 60 seconds you must use Kinesis Data Streams |
| What is the default consistency model for DAX in-memory cache? | Eventual |
| Can you utilize multiple DAX clusters for the same DynamoDB table? | Yes |
| If you need to load data into an Amazon Redshift cluster from Amazon S3, usingthe COPY command, what is the most efficient way to detect load errors without performing anycleanup if the load process fails? | If you need to load data into an Amazon Redshift cluster from Amazon S3, using<br>the COPY command, what is the most efficient way to detect load errors without performing any<br>cleanup if the load process fails?Use COPY with NOLOAD parameter. |
| If you have a nightly COPY command running to load S3 data into a Redshift cluster, how can you optimize performance of the COPY command? | Split the file into as many small files as possible to improve parallelism<br><br>(and always use a manifest file) |
| What are 3 ways AWS can interact with on-prem Active Directory? | IAM Federation <br>AD Synchronization service<br>AD FS (Active Directory Federation Services) |
| If you need a data store to handle the following data types and access patterns, what data store should you use?- Key-value access pattern- Complex SQL queries and transactions- Consistent reads- Fixed schema | If you need a data store to handle the following data types and access patterns, what data store should you use?<br><br>- Key-value access pattern<br>- Complex SQL queries and transactions<br>- Consistent reads<br>- Fixed schemaAmazon RDS |
| If you have an application that emits multiple types of events to Kinesis Data Streams for operational reporting, wherein, critical events must be captured immediately before processing can continue, but informational events do not need to delay processing, what should you do? | Log critical events using the PutRecords API method, and log informational events using the Kinesis Producer Library. |
| If you have a mobile application which collects data that must be securely stored in multiple Availability Zones within five minutes of being captured in the app, what should you use? | The mobile app should authenticate with a Cognito identity that is authorized to write to Kinesis Data Firehose with an S3 destination. |
| If you need to collect data from multiple Redshift clusters and consolidate the data into a single central data warehouse, but data must be encrypted at all times while at rest or in flight, what is the most scalable way to do this? | Use AWS KMS data key to run an UNLOAD ENCRYPTED command that stores the data in an unencrypted S3 bucket; run a COPY command to move the data into the target cluster. |
| If you have logs you need to keep for 7 years for regulatory reasons, which are accessed frequently in the first 3 months, then rarely up to a year, what should you do? | Store the files in S3 Standard with lifecycle policies to transition the storage class to Standard - IA after three months and delete them after a year. Simultaneously store the files in Amazon Glacier with a Deny Delete vault lock policy for archives less than seven years old |
| If you're using Redshift and need to ensure that data is encrypted at rest and that the keys are managed by a corporate on-premises HSM, what should you do? | Create a VPC, and then establish a VPN connection between the VPC and the on-premises network.<br>Launch the Redshift cluster in the VPC, and configure it to use your corporate HSM. |
| Are KMS keys specific to a region? | Yes |
| Describe the functions of the 3 Redshift distribution keys... | Even distribution:<br>This is the default distribution styles of a table. In Even Distribution the Leader node of the cluster distributes the data of a table evenly across all slices, using a round robin approach.<br><br>Key distribution: <br>The data is distributed across slices by the leader node matching the values of a designated column. So all the entries with the same value in the column end up in the same slice.<br><br>All distribution:<br>Leader node maintains a copy of the table on all the computing nodes resulting in more space utilisation. Since all the nodes have a local copy of the data, the query does not require copying data across the network. This results in faster query operations. The negative side of using ALL is that a copy of the table is on every node in the cluster. This takes up too much of space and increases the time taken by Copy command to upload data into Redshift. |
| What existing database technology was Redshift built from? | Amazon Redshift is based on an older version of PostgreSQL 8.0.2, and Redshift has made changes to that version. |

